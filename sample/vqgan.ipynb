{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import argparse\n",
    "import yaml\n",
    "import importlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from utils_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\AppData\\Roaming\\Python\\Python37\\site-packages\\urllib3\\util\\selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import namedtuple, Mapping\n",
      "C:\\Users\\lucky\\AppData\\Roaming\\Python\\Python37\\site-packages\\urllib3\\_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from C:\\MyFiles\\CondTran\\frameworks\\bert_baseline\\vqgan\\models\\..\\data\\lpips\\vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Working with z of shape (1, 1024, 16, 16) = 262144 dimensions.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "\n",
    "parser.add_argument('--root_dir', type=str, default='C:/MyFiles/CondTran/frameworks/bert_baseline')\n",
    "parser.add_argument('--log_dir', type=str, default='logs/vqgan_imagenet_full')\n",
    "parser.add_argument('--step', type=str, default='279999')\n",
    "args = parser.parse_args(args=[])\n",
    "os.chdir(args.root_dir)\n",
    "sys.path.append(args.root_dir)\n",
    "\n",
    "def find(path, name):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if name in f:\n",
    "                return os.path.join(root, f)\n",
    "args.checkpoint = find(args.log_dir, args.step+'.ckpt')\n",
    "\n",
    "module = importlib.import_module(f'vqgan.models')\n",
    "vq_model = getattr(importlib.import_module(f'vqgan.models.hybrid_vqgan'), 'VQModel')\n",
    "vq_loss = getattr(importlib.import_module(f'vqgan.models.vqperceptual'), 'VQLPIPSWithDiscriminator')\n",
    "\n",
    "config_path = os.path.join(args.log_dir, 'config.yaml')\n",
    "with open(config_path, 'rb') as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "\n",
    "# Load pretrained model\n",
    "vqgan_model = vq_model.load_from_checkpoint(\n",
    "    args.checkpoint,\n",
    "    ddconfig=config['model']['ddconfig'],\n",
    "    loss=vq_loss(**config['loss']),\n",
    "    n_embed=config['model']['n_embed'],\n",
    "    embed_dim=config['model']['embed_dim'],\n",
    "    learning_rate=0.0,\n",
    ").to(args.device).eval().requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:03,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1491]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'C:/MyFiles/ColorizationTran/data/raw'\n",
    "img_size = [256, 256]\n",
    "\n",
    "pbar = tqdm(enumerate(os.listdir(img_dir)))\n",
    "for i, filename in pbar:\n",
    "    #if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.JPEG') or filename.endswith('.jpeg'):\n",
    "    if 'in3' in filename:\n",
    "        I_color = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "        I_gray = I_color.convert('L')\n",
    "\n",
    "\n",
    "        #I_color = draw_color(I_gray, [255, 50, 10], [None, None, None, None])\n",
    "\n",
    "        x_color = preprocess(I_color, img_size).to(args.device)\n",
    "        x_gray = preprocess(I_gray, img_size).to(args.device)\n",
    "\n",
    "        x_color = x_color[:, :, 32:48, 48:64]\n",
    "        x_gray = x_gray[:, :, 32:48, 48:64]\n",
    "        # Encoding\n",
    "        f_gray = vqgan_model.gray_encoder(x_gray)\n",
    "        h = vqgan_model.encoder(x_color)\n",
    "        h = vqgan_model.quant_conv(h)\n",
    "        quant, emb_loss, info = vqgan_model.quantize(h)\n",
    "        color_idx = info[2].view(quant.shape[0], -1)\n",
    "        # Decoding\n",
    "        q_shape = [f_gray.shape[0], f_gray.shape[2], f_gray.shape[3]]\n",
    "        quant = vqgan_model.quantize.get_codebook_entry(color_idx.view(-1), q_shape)\n",
    "        feat = torch.cat([quant, f_gray], dim=1)\n",
    "        feat = vqgan_model.post_quant_conv(feat)\n",
    "        rec = vqgan_model.decoder(feat)\n",
    "\n",
    "        output_to_pil(x_color[0]).show()\n",
    "        output_to_pil(rec[0]).show()\n",
    "\n",
    "        print(color_idx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
