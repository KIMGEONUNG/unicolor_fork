{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import importlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import json\n",
    "import nltk\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "import clip\n",
    "import kornia\n",
    "import torchvision\n",
    "import skimage.color\n",
    "import random\n",
    "\n",
    "from utils_func import *\n",
    "from html_images import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 1024, 16, 16) = 262144 dimensions.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "\n",
    "parser.add_argument('--root_dir', type=str, default='C:/MyFiles/CondTran/finals/bert_final')\n",
    "parser.add_argument('--log_dir', type=str, default='logs/bert')\n",
    "parser.add_argument('--step', type=str, default='142124')\n",
    "\n",
    "# Load transformer\n",
    "args = parser.parse_args(args=[])\n",
    "os.chdir(args.root_dir)\n",
    "sys.path.append(args.root_dir)\n",
    "module = importlib.import_module(f'filltran.models.colorization')\n",
    "args.fill_model = getattr(module, 'Colorization')\n",
    "filltran = load_model(args.fill_model, args.log_dir, args.step).to(args.device).eval().requires_grad_(False)\n",
    "\n",
    "def rgb_to_L(img):\n",
    "    img = np.array(img)\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l = lab[:, :, 0]\n",
    "    return Image.fromarray(l)\n",
    "\n",
    "def resize_color(l, rgb):\n",
    "    rgb = np.array( rgb.resize(l.size) )\n",
    "    l = np.array(l)\n",
    "    l = np.expand_dims(l, axis=2)\n",
    "    ab = cv2.cvtColor(rgb, cv2.COLOR_RGB2LAB)[:, :, 1:3]\n",
    "    \n",
    "    lab = np.concatenate([l, ab], axis=2)\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'C:\\\\MyFiles\\\\CondTran\\\\data\\\\raw\\\\in1.JPEG'\n",
    "I_color = Image.open(img_path).convert('RGB')\n",
    "I_gray = I_color.convert('L')\n",
    "I_gray.save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\gray.png')\n",
    "I_color.save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\original.png')\n",
    "\n",
    "x_color = preprocess(I_color, None).to(args.device)\n",
    "x_gray = preprocess(I_gray, None).to(args.device)\n",
    "\n",
    "idx_color, f_gray = filltran.hybrid_vqgan.encode(x_color, x_gray)\n",
    "idx_color = idx_color.reshape(idx_color.shape[0], f_gray.shape[2], f_gray.shape[3])\n",
    "#idx_color = torch.randint(low=0, high=4097, size=idx_color.shape).long().to(args.device)\n",
    "x_rec = filltran.hybrid_vqgan.decode(idx_color, f_gray)\n",
    "I_rec = output_to_pil(x_rec[0])\n",
    "I_rec.save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\recon.png')\n",
    "\n",
    "x_color[:, 0, :, :] = 0.0; x_color[:, 1, :, :] = -0.6; x_color[:, 2, :, :] = -0.6\n",
    "idx_color, f_gray = filltran.hybrid_vqgan.encode(x_color, x_gray)\n",
    "idx_color = idx_color.reshape(idx_color.shape[0], f_gray.shape[2], f_gray.shape[3])\n",
    "#idx_color = torch.randint(low=0, high=4097, size=idx_color.shape).long().to(args.device)\n",
    "x_rec = filltran.hybrid_vqgan.decode(idx_color, f_gray)\n",
    "output_to_pil(x_rec[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\recon_r.png')\n",
    "output_to_pil(x_color[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\r.png')\n",
    "\n",
    "x_color[:, 0, :, :] = -0.6; x_color[:, 1, :, :] = 0.0; x_color[:, 2, :, :] = -0.6\n",
    "idx_color, f_gray = filltran.hybrid_vqgan.encode(x_color, x_gray)\n",
    "idx_color = idx_color.reshape(idx_color.shape[0], f_gray.shape[2], f_gray.shape[3])\n",
    "#idx_color = torch.randint(low=0, high=4097, size=idx_color.shape).long().to(args.device)\n",
    "x_rec = filltran.hybrid_vqgan.decode(idx_color, f_gray)\n",
    "output_to_pil(x_rec[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\recon_g.png')\n",
    "output_to_pil(x_color[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\g.png')\n",
    "\n",
    "x_color[:, 0, :, :] = -0.6; x_color[:, 1, :, :] = -0.6; x_color[:, 2, :, :] = 0.0\n",
    "idx_color, f_gray = filltran.hybrid_vqgan.encode(x_color, x_gray)\n",
    "idx_color = idx_color.reshape(idx_color.shape[0], f_gray.shape[2], f_gray.shape[3])\n",
    "#idx_color = torch.randint(low=0, high=4097, size=idx_color.shape).long().to(args.device)\n",
    "x_rec = filltran.hybrid_vqgan.decode(idx_color, f_gray)\n",
    "output_to_pil(x_rec[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\recon_b.png')\n",
    "output_to_pil(x_color[0]).save('C:\\\\MyFiles\\\\CondTran\\\\sample\\\\results\\\\b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-58\n"
     ]
    }
   ],
   "source": [
    "img_path = 'C:\\\\MyFiles\\\\CondTran\\\\data\\\\stroke\\\\3.JPEG'\n",
    "I_color = Image.open(img_path).convert('RGB')\n",
    "I_gray = I_color.convert('L')\n",
    "I_l = rgb_to_L(I_color)\n",
    "\n",
    "diff = np.array(I_gray).astype(int) - np.array(I_l).astype(int)\n",
    "\n",
    "print(np.max(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9\n"
     ]
    }
   ],
   "source": [
    "img_path = 'C:\\\\MyFiles\\\\CondTran\\\\data\\\\stroke\\\\3.JPEG'\n",
    "I_color = Image.open(img_path).convert('RGB')\n",
    "I_gray = I_color.convert('L')\n",
    "\n",
    "I_l = rgb_to_L(I_gray.convert('RGB'))\n",
    "\n",
    "diff = np.array(I_gray).astype(int) - np.array(I_l).astype(int)\n",
    "\n",
    "I_gray.show()\n",
    "I_l.show()\n",
    "\n",
    "print(np.min(diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d703c633b54b2fa5838b43c4e4963eee49000d73a8d9f4372c69dbdf8920ce3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
